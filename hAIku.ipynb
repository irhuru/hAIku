{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hAIku.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWkhsU54K4RC"
      },
      "source": [
        "#Import and install Python libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nGL4wmbM7N3"
      },
      "source": [
        "import spacy.cli\n",
        "from spacy.matcher import Matcher\n",
        "import base64\n",
        "import requests\n",
        "!pip install syllapy\n",
        "import syllapy\n",
        "import random\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axDPZzyjLIfM"
      },
      "source": [
        "#Import text data for the haiku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve8fsWlgKueR"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/irhuru/hAIku/main/texts_data.txt\"\n",
        "req = requests.get(url)\n",
        "texts = req.text\n",
        "print(texts[0:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b85-kTuHLZ8S"
      },
      "source": [
        "#Choose valid patterns and identify them in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEvhh8ZPEfEf"
      },
      "source": [
        "# Groups of 2 words\n",
        "\n",
        "matcher2w = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"POS\":{\"IN\": [\"NOUN\", \"ADV\", \"ADJ\", \"ADP\"]}},\n",
        "           {\"POS\":{\"IN\": [\"VERB\", \"NOUN\"]}}]\n",
        "matcher2w.add(\"2words\", [pattern])\n",
        "\n",
        "# Groups of 3 words\n",
        "\n",
        "matcher3w = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"POS\":{\"IN\": [\"NOUN\", \"ADV\", \"ADJ\", \"VERB\", \"ADP\"]}},\n",
        "           {\"IS_ASCII\": True, \"IS_PUNCT\": False},\n",
        "           {\"POS\":{\"IN\": [\"VERB\", \"NOUN\", \"ADV\", \"ADJ\"]}}]\n",
        "matcher3w.add(\"3words\", [pattern])\n",
        "\n",
        "# Groups of 4 words\n",
        "\n",
        "matcher4w = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"POS\":{\"IN\": [\"NOUN\", \"ADV\", \"ADJ\", \"VERB\", \"ADP\"]}},\n",
        "           {\"IS_ASCII\": True, \"IS_PUNCT\": False},\n",
        "           {\"IS_ASCII\": True, \"IS_PUNCT\": False},\n",
        "           {\"POS\":{\"IN\": [\"VERB\", \"NOUN\", \"ADV\", \"ADJ\"]}}]\n",
        "matcher4w.add(\"4words\", [pattern])\n",
        "\n",
        "# Identify patterns in the text\n",
        "\n",
        "doc = nlp(texts)\n",
        "matches2w = matcher2w(doc)\n",
        "matches3w = matcher3w(doc)\n",
        "matches4w = matcher4w(doc)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5e4KG80MEZl"
      },
      "source": [
        "#Create lists containing valid lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTPsXJptL_aG"
      },
      "source": [
        "lines_5_syll = []\n",
        "lines_7_syll = []\n",
        "\n",
        "for match_id, start, end in matches2w + matches3w + matches4w:\n",
        "    string_id = nlp.vocab.strings[match_id] \n",
        "    span = doc[start:end]\n",
        "\n",
        "    syllable_count = 0\n",
        "    \n",
        "    for token in span:\n",
        "      syllable_count += syllapy.count(token.text)\n",
        "    if syllable_count == 5:\n",
        "      if span.text not in lines_5_syll:\n",
        "        lines_5_syll.append(span.text)\n",
        "    if syllable_count == 7:\n",
        "      if span.text not in lines_7_syll:\n",
        "        lines_7_syll.append(span.text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOq0C_o3MiMs"
      },
      "source": [
        "#Print a random haiku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dARhOlt3VV_9",
        "outputId": "6fc7e19d-779e-4a9e-fe6b-82910694635f"
      },
      "source": [
        "print(\"{0}\\n{1}\\n{2}\".format(random.choice(lines_5_syll), random.choice(lines_7_syll), random.choice(lines_5_syll)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for their own faces\n",
            "glittering teeth resembling\n",
            "greatest power on earth\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}